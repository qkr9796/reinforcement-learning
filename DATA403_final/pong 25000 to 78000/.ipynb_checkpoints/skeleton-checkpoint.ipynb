{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gymnasium\n",
    "#pip install gymnasium[box2d] gymnasium[mujoco] gymnasium[atari] gymnasium[accept-rom-license]\n",
    "#pip install omeaconf, hydra-core\n",
    "#pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.experimental.wrappers import RecordVideoV0 as RecordVideo # warppers for making game video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- Selecting environments\n",
    "- Implementing PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Select your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select your environment from the list\n",
    "\"\"\"\n",
    "environments list  \n",
    "classic control env_id list:  [\"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]  \n",
    "box2d env_id list:            [\"LunarLander-v2\", \"BipedalWalker-v3\"]  \n",
    "mujoco env_id list:           [\"Swimmer-v4\" , \"Reacher-v4\", \"Hopper-v4\", \"Walker2d-v4\", \"Ant-v4\", \"HalfCheetah-v4\", \"HumanoidStandup-v4\"]  \n",
    "atari env_id list:            [\"BreakoutNoFrameskip-v4\", \"MsPacmanNoFrameskip-v4\", \"PongNoFrameskip-v4\"] (optional. not recommended for no gpu device)\n",
    "\"\"\"\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco_env_id = [\"Swimmer-v4\" , \"Reacher-v4\", \"Hopper-v4\", \"Walker2d-v4\", \"Ant-v4\", \"HalfCheetah-v4\", \"HumanoidStandup-v4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 experiment config, path config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to change the experiment configurations except `max_episode_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config = OmegaConf.create({\n",
    "    \"seed\": 0, # environment seed\n",
    "    \"num_envs\": 8, # the number of environments for parallel training\n",
    "    \"num_eval\": 10, # the number of evaluations\n",
    "    \"max_episode_steps\": 2048, # the maximum number of episode steps. Used in mujoco environments ! Don't change this value\n",
    "    \"num_rollout_steps\": 128, # the number of policy rollout steps\n",
    "    \"num_minibatches\": 4, # The number of minibatches per 1 epoch (Not mibi batch size)\n",
    "    \"total_timesteps\": 10000000, # total number of frames\n",
    "    \"print_interval\": 100, # print iverval of episodic return\n",
    "    \"early_stop_wating_steps\": 5000, # early stopping steps\n",
    "})\n",
    "\n",
    "path_config = OmegaConf.create({\n",
    "  \"logs\": Path(\"./runs\"),\n",
    "  \"videos\":Path(\"./videos\"),\n",
    "  \"checkpoints\": Path(\"./checkpoints\"),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 make_env function\n",
    "For vectorized environments, we need a callable make_env function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, exp_config, path_config: OmegaConf, evaluation=False, idx=0):\n",
    "    video_path = Path(path_config.videos)\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        if evaluation:\n",
    "            test_path = Path(f\"{env_id}/test\")\n",
    "            video_save_path = str(video_path / test_path)\n",
    "        else:\n",
    "            train_path = Path(f\"{env_id}/train\")\n",
    "            video_save_path = str(video_path / train_path)\n",
    "        if idx==0:\n",
    "            if evaluation:\n",
    "                env = RecordVideo(env, video_save_path, disable_logger=True, episode_trigger= lambda x : True)\n",
    "            else:\n",
    "                env = RecordVideo(env, video_save_path, disable_logger=True)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if env_id in mujoco_env_id:\n",
    "            env = gym.wrappers.TimeLimit(env, exp_config.max_episode_steps)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.make_env.<locals>.thunk()>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_env(env_id, exp_config, path_config) # the return of make_env is callable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env_id, exp_config, path_config)() # <- Note that () is call action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files (x86)\\python\\lib\\site-packages\\gymnasium\\experimental\\wrappers\\rendering.py:169: UserWarning: \u001b[33mWARN: Overwriting existing videos at E:\\Joes\\course\\reinforcement learning\\DATA403_final(1)\\pong\\videos\\PongNoFrameskip-v4\\train folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorized environments for fast training\n",
    "# https://gymnasium.farama.org/api/vector/\n",
    "envs = gym.vector.SyncVectorEnv(make_env(env_id, exp_config, path_config, evaluation=False, idx=idx)\n",
    "                                for idx in range(exp_config.num_envs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that whether the environment is discrete or continuous\n",
    "`gymnasium.spaces.Box`: continuous space   \n",
    "`gynmasium.spaces.Discrete`: discrete space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (210, 160, 3), uint8)\n",
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. make configurations\n",
    "- environment\n",
    "- ppo hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 make environment config\n",
    "This configuration store the information of environment to build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "def make_env_config(envs):\n",
    "    env = envs.envs[0] \n",
    "    print(env.observation_space)\n",
    "    print(env.action_space)\n",
    "    \n",
    "    # * observation information\n",
    "    if isinstance(env.observation_space, Discrete): # if observation_space is discrete\n",
    "        state_dim = env.observation_space.n\n",
    "    \n",
    "    else:  # if observation_space is continuous\n",
    "        if len(env.observation_space.shape) > 1: # Atari visual observation case\n",
    "            state_dim = env.observation_space.shape\n",
    "        else: # 1D vector observation case (classic control, box2d, mujoco)\n",
    "            state_dim = env.observation_space.shape[0]\n",
    "    \n",
    "    # * action_space information\n",
    "    num_discretes = 0\n",
    "    if isinstance(env.action_space, Box):\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        is_continuous = True\n",
    "    elif isinstance(env.action_space, Discrete):\n",
    "        action_dim = 1\n",
    "        num_discretes = env.action_space.n\n",
    "        is_continuous = False\n",
    "    env_config = OmegaConf.create({\"state_dim\": state_dim,\n",
    "                    \"action_dim\": int(action_dim),\n",
    "                    \"num_discretes\": int(num_discretes),\n",
    "                    \"is_continuous\": is_continuous})\n",
    "    return env_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (210, 160, 3), uint8)\n",
      "Discrete(6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'state_dim': [210, 160, 3], 'action_dim': 1, 'num_discretes': 6, 'is_continuous': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_config = make_env_config(envs)\n",
    "env_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ppo config\n",
    "This configuration store the information of hyperparameters for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to change the ppo configurations depending on selected environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anneal_lr': False, 'update_epochs': 4, 'minibatch_size': 256, 'lr': 0.00025, 'max_grad_norm': 0.5, 'norm_adv': True, 'clip_coef': 0.1, 'ent_coef': 0.01, 'vf_coef': 0.5, 'gamma': 0.99, 'gae_lambda': 0.95}\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "ppo_config = OmegaConf.create({\n",
    "    \"anneal_lr\": False,\n",
    "    \"update_epochs\": 4, # The number of iteractions of ppo training\n",
    "    \"minibatch_size\": 256, \n",
    "    \"lr\": 2.5e-4,\n",
    "    \"max_grad_norm\": 0.5, \n",
    "    \"norm_adv\": True,\n",
    "    \"clip_coef\": 0.1,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    })\n",
    "print(ppo_config)\n",
    "print(ppo_config.minibatch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementing PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should do the following to implement PPO.\n",
    "- Complete the ActorCritic class\n",
    "- Implement the generalized advatage calculation part\n",
    "- Implement the PPO loss and Value loss calculation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ActorCritic Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can create a neural network as you want to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)   \n",
    "    return layer\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.state_dim = env_config[\"state_dim\"]\n",
    "        self.action_dim = env_config[\"action_dim\"]\n",
    "        self.num_discretes = env_config[\"num_discretes\"]\n",
    "        self.is_continuous = env_config[\"is_continuous\"]\n",
    "        \n",
    "        ###################### Implement here : 1. Neural Network ########################\n",
    "        d = 64\n",
    "        self.sharedNet = nn.Sequential(\n",
    "                        layer_init(nn.Conv2d(1, d, 8, 4, 0)),\n",
    "                        nn.ReLU(),\n",
    "                        layer_init(nn.Conv2d(d, d, 4, 2, 0)),\n",
    "                        nn.ReLU(),\n",
    "                        layer_init(nn.Conv2d(d, d, 3, 1, 0)),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Flatten(),\n",
    "                        layer_init(nn.Linear(d*7*7, d)),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        self.critic = layer_init(nn.Linear(d, 1))\n",
    "        \n",
    "        if self.is_continuous:\n",
    "            self.actor_mean = layer_init(nn.Linear(d, self.action_dim))\n",
    "            \n",
    "            self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_dim))\n",
    "        \n",
    "        else:\n",
    "            self.actor_logit = nn.Sequential(\n",
    "                                layer_init(nn.Linear(d, self.action_dim * self.num_discretes)),\n",
    "                                nn.Softmax(dim=-1))\n",
    "    \n",
    "            \n",
    "    def get_value(self, x):\n",
    "        x = self.sharedNet(x)\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        ###################### Implement here : policy distribution ########################\n",
    "        x = self.sharedNet(x)\n",
    "        if self.is_continuous:\n",
    "            action_mean = self.actor_mean(x)\n",
    "            action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "            action_std = torch.exp(action_logstd)\n",
    "            probs = Normal(action_mean, action_std) # Use torch distribution Noraml \n",
    "            if action is None:\n",
    "                action = probs.sample()\n",
    "            return action, probs.log_prob(action).sum(-1), probs.entropy().sum(-1), self.critic(x)\n",
    "        else:\n",
    "            logits = self.actor_logit(x)\n",
    "            probs = Categorical(logits) # Use torch distribution Categorical\n",
    "            if action is None:\n",
    "                action = probs.sample()\n",
    "            return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "        \n",
    "        \n",
    "def save_model(env_id, path_cfg, actor_critic, update):\n",
    "    ckpt_path = Path(path_cfg.checkpoints) / Path(f\"{env_id}\")\n",
    "    if not ckpt_path.exists():\n",
    "        ckpt_path.mkdir()\n",
    "    model_name = Path(f\"PPO_{update}.pt\")\n",
    "    model_path = ckpt_path / model_name\n",
    "    torch.save(actor_critic.state_dict(), str(model_path))\n",
    "    print(f\"model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class GlobalLogger:\n",
    "    global_steps: List\n",
    "    save_update_steps: List\n",
    "    episodic_return_steps: List\n",
    "    train_episodic_return: List \n",
    "    test_episodic_return: List \n",
    "    policy_loss: List\n",
    "    value_loss: List\n",
    "    entropy_loss: List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the GAE calculation part and ppo loss, value loss part by referring the pictures.\n",
    "\n",
    "### GAE: https://arxiv.org/abs/1506.02438\n",
    "\n",
    "\n",
    "### PPO: https://arxiv.org/abs/1707.06347\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](GAE_calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO Clipped loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](PPO_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can refer to any open source code and complete it. \n",
    "GAE references:  \n",
    "- https://towardsdatascience.com/generalized-advantage-estimate-maths-and-code-b5d5bd3ce737  \n",
    "- https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/  \n",
    "\n",
    "PPO references:  \n",
    "- https://spinningup.openai.com/en/latest/algorithms/ppo.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "266dbc1b53b3a75132c42213de358c469c42e7e8486df6091cb08cba7b9be0d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
